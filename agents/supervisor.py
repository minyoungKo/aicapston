from dotenv import load_dotenv
load_dotenv()
from typing import TypedDict, Optional, List, Dict
import asyncio
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from tools.wrapped_agents import (
    invoke_stock_info_collector,
    invoke_news_analyzer,
    invoke_chart_analyzer,
    invoke_fundamental
)

llm = ChatOpenAI(model="gpt-4o", temperature=0)

# ìƒíƒœ ìŠ¤í‚¤ë§ˆ ì •ì˜
class SupervisorState(TypedDict):
    input: str
    routes: List[str]
    results: Dict[str, str]
    report: Optional[str]

# ë¼ìš°í„° ë…¸ë“œ
async def multi_router_node(state: SupervisorState) -> SupervisorState:
    user_input = state["input"]
    prompt = (
        "ë‹¤ìŒ ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•´ ì–´ë–¤ ë„êµ¬ë“¤ì„ í•¨ê»˜ ì‚¬ìš©í• ì§€ íŒë‹¨í•˜ì„¸ìš”.\n"
        "ë°˜ë“œì‹œ ì•„ë˜ ë„êµ¬ ì´ë¦„ë“¤ ì¤‘ í•„ìš”í•œ ê²ƒë§Œ ê³¨ë¼ì„œ Python ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.\n"
        "- stock_info_collector: ì£¼ê°€, ì‹œì„¸, ê±°ë˜ëŸ‰ ë“±\n"
        "- news_analyzer: ì¢…ëª© ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë¶„ì„\n"
        "- chart_analyzer: ì°¨íŠ¸ ê¸°ë°˜ ê¸°ìˆ ì  ë¶„ì„\n"
        "- fundamental_analyzer: ì¬ë¬´ì œí‘œ ê¸°ë°˜ ë¶„ì„(ë§¤ì¶œì•¡,ë‹¹ê¸°ìˆœì´ìµ,ì¬ë¬´ìƒíƒœí‘œ,í˜„ê¸ˆíë¦„í‘œ,ì†ìµê³„ì‚°ì„œ)\n\n"
        f"ì…ë ¥: {user_input}\n"
        "ì¶œë ¥ ì˜ˆì‹œ: ['stock_info_collector', 'chart_analyzer']"
    )
    response = await llm.ainvoke(prompt)
    try:
        routes = eval(response.content.strip())
    except:
        routes = []
    return {"routes": routes, "input": user_input, "results": {}, "report": None}

# ë³‘ë ¬ ì‹¤í–‰ í•¨ìˆ˜
async def run_parallel_tools(user_input: str, routes: list[str]) -> dict:
    route_to_func = {
        "stock_info_collector": invoke_stock_info_collector,
        "chart_analyzer": invoke_chart_analyzer,
        "news_analyzer": invoke_news_analyzer,
        "fundamental_analyzer": invoke_fundamental,
    }

    coroutines = []
    for route in routes:
        func = route_to_func.get(route)
        if func:
            coroutine = func(user_input)
            coroutines.append((route, coroutine))

    results_raw = await asyncio.gather(
        *[c for _, c in coroutines], return_exceptions=True
    )

    results = {}
    for (route, _), result in zip(coroutines, results_raw):
        if isinstance(result, Exception):
            results[route] = f"[{route}] ì—ëŸ¬: {str(result)}"
        else:
            results[route] = result

    return results

# ë³‘ë ¬ ì‹¤í–‰ ë…¸ë“œ
async def parallel_node(state: SupervisorState) -> SupervisorState:
    results = await run_parallel_tools(state["input"], state["routes"])
    return {
        "input": state["input"],
        "routes": state["routes"],
        "results": results,
        "report": None  # ì´ì œ reportëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
    }

# ê·¸ë˜í”„ êµ¬ì„±
builder = StateGraph(SupervisorState)

builder.add_node("router", multi_router_node)
builder.add_node("parallel_executor", parallel_node)

builder.set_entry_point("router")
builder.add_edge("router", "parallel_executor")
builder.set_finish_point("parallel_executor")

# ì»´íŒŒì¼
defined_supervisor_graph = builder.compile()

# ì‹¤í–‰ ì˜ˆì‹œ (ë¹„ë™ê¸°)
if __name__ == "__main__":
    async def main():
        while True:
            query = input("\në¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ")
            if query.lower() in ["exit", "quit"]:
                break
            result = await defined_supervisor_graph.ainvoke({"input": query})
            print("\n[ê°œë³„ ë¶„ì„ ê²°ê³¼]")
            for key, value in result["results"].items():
                print(f"\nğŸ”¹ {key}\n{value}")

    asyncio.run(main())
